{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "2a1cbf41-2628-407b-ae3b-8414a2cc4fc0",
      "cell_type": "code",
      "source": "# ----------------------------\n# 1. IMPORT LIBRARIES\n# ----------------------------\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, losses\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"TensorFlow version:\", tf.__version__)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f4e6f0f0-936b-4051-a1b1-7de97c91cfb4",
      "cell_type": "code",
      "source": "# ----------------------------\n# 2. LOAD AND PREPROCESS DATA\n# ----------------------------\n(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n\n# Normalize to [-1, 1]\nx_train = (x_train.astype('float32') - 127.5) / 127.5\nx_train = np.expand_dims(x_train, axis=-1)\n\nprint(\" Data shape:\", x_train.shape)\nprint(\" Pixel range:\", x_train.min(), \"to\", x_train.max())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bf7a9246-e20b-48db-b3fb-6c400a9561b6",
      "cell_type": "code",
      "source": "# ----------------------------\n# 3. BUILD GENERATOR\n# ----------------------------\ndef build_generator():\n    model = models.Sequential([\n        # First dense layer\n        layers.Dense(7*7*256, input_dim=100),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(0.2),\n        \n        # Reshape to 7x7x256\n        layers.Reshape((7, 7, 256)),\n        \n        # Upsample to 14x14\n        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(0.2),\n        \n        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(0.2),\n        \n        # Upsample to 28x28\n        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n    ])\n    return model\n\ngenerator = build_generator()\nprint(\" Generator built:\")\ngenerator.summary()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "318fc4ca-1343-43a1-9d50-5e4affc9f559",
      "cell_type": "code",
      "source": "# ----------------------------\n# 4. BUILD DISCRIMINATOR\n# ----------------------------\ndef build_discriminator():\n    model = models.Sequential([\n        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]),\n        layers.LeakyReLU(0.2),\n        layers.Dropout(0.3),\n        \n        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(0.2),\n        layers.Dropout(0.3),\n        \n        layers.Flatten(),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    return model\n\ndiscriminator = build_discriminator()\nprint(\" Discriminator built:\")\ndiscriminator.summary()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eed2c5ef-a68c-433d-bd49-dcbee03e8e74",
      "cell_type": "code",
      "source": "# ----------------------------\n# 5. COMPILE MODELS\n# ----------------------------\n# Optimizer\nopt = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n\n# Discriminator\ndiscriminator.compile(\n    optimizer=opt,\n    loss=losses.BinaryCrossentropy(),\n    metrics=['accuracy']\n)\n\n# GAN (combined model)\ndiscriminator.trainable = False\ngan_input = layers.Input(shape=(100,))\ngan_output = discriminator(generator(gan_input))\ngan = models.Model(gan_input, gan_output)\ngan.compile(\n    optimizer=opt,\n    loss=losses.BinaryCrossentropy()\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "097a1d5b-286b-4585-86c1-47aaac388469",
      "cell_type": "code",
      "source": "# ----------------------------\n# 6. TRAINING LOOP\n# ----------------------------\nEPOCHS = 20\nBATCH_SIZE = 128\nnoise_dim = 100\n\n# For visualization\nfixed_noise = tf.random.normal([16, noise_dim])\n\n# Track losses\ngen_losses = []\ndisc_losses = []\n\nfor epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n    \n    epoch_gen_loss = 0\n    epoch_disc_loss = 0\n    num_batches = 0\n    \n    for i in tqdm(range(0, len(x_train), BATCH_SIZE)):\n        # Get real batch\n        real_batch = x_train[i:i+BATCH_SIZE]\n        batch_size = real_batch.shape[0]\n        \n        # Train Discriminator\n        noise = tf.random.normal([batch_size, noise_dim])\n        fake_batch = generator(noise, training=False)\n        \n        real_labels = tf.ones((batch_size, 1))\n        fake_labels = tf.zeros((batch_size, 1))\n        \n        # Combine real and fake\n        all_images = tf.concat([real_batch, fake_batch], axis=0)\n        all_labels = tf.concat([real_labels, fake_labels], axis=0)\n        \n        # Add label smoothing\n        all_labels += 0.05 * tf.random.uniform(all_labels.shape)\n        \n        d_loss = discriminator.train_on_batch(all_images, all_labels)\n        \n        # Train Generator\n        noise = tf.random.normal([batch_size, noise_dim])\n        g_labels = tf.ones((batch_size, 1))\n        g_loss = gan.train_on_batch(noise, g_labels)\n        \n        epoch_gen_loss += g_loss\n        epoch_disc_loss += d_loss[0]\n        num_batches += 1\n    \n    # Average losses\n    avg_g_loss = epoch_gen_loss / num_batches\n    avg_d_loss = epoch_disc_loss / num_batches\n    gen_losses.append(avg_g_loss)\n    disc_losses.append(avg_d_loss)\n    \n    print(f\"Generator Loss: {avg_g_loss:.4f}, Discriminator Loss: {avg_d_loss:.4f}\")\n    \n    # Generate and save images every 5 epochs\n    if (epoch + 1) % 5 == 0:\n        generated = generator(fixed_noise, training=False)\n        plt.figure(figsize=(8, 8))\n        for i in range(16):\n            plt.subplot(4, 4, i+1)\n            plt.imshow(generated[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n            plt.axis('off')\n        plt.suptitle(f'Generated Digits - Epoch {epoch+1}')\n        plt.savefig(f'gan_epoch_{epoch+1}.png')\n        plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cdbddeb4-2c0f-4797-9258-f0d0c56d3ad7",
      "cell_type": "code",
      "source": "# ----------------------------\n# 7. PLOT LOSS CURVES\n# ----------------------------\nplt.figure(figsize=(10, 6))\nplt.plot(gen_losses, label='Generator Loss', color='blue')\nplt.plot(disc_losses, label='Discriminator Loss', color='red')\nplt.title('GAN Training Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('gan_loss_curve.png')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1427dfa7-d7d2-4498-99ca-47101a3fdd27",
      "cell_type": "code",
      "source": "# Generate final samples\nfinal_noise = tf.random.normal([25, noise_dim])\nfinal_generated = generator(final_noise, training=False)\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(final_generated[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n    plt.axis('off')\nplt.suptitle('Final Generated Digits (25 samples)')\nplt.savefig('gan_final_samples.png')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ec39a9e1-a411-4869-bd1b-1dddfdf6e71b",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}